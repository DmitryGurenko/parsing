{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import csv\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CSV = 'xxxxxx.csv'   # the name of the file in which the data will be imported\n",
    "HOST = \"https://xxxxxxx\"  # the site from which you will receive data\n",
    "URL = \"https://xxxxxxx\" # directory path on the site from where to import data\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# your browser data so that the server does not accept you as a bot, \n",
    "you can take this data in the browser settings as a developer\n",
    "\"\"\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"user-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:86.0) Gecko/20100101 Firefox/86.0\"} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_html(url, params=''):\n",
    "    \n",
    "    \"\"\"\n",
    "    request function\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    r = session.get(url, headers=HEADERS, params=params)\n",
    "    \n",
    "    return r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data(html):\n",
    "    \n",
    "    \"\"\"\n",
    "    functions for parsing data from the catalog, \n",
    "    you can find this data in the browser menu for the developer\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    elements = soup.find_all('div', class_= 'sku-card-small-container') # selecting the main data class\n",
    "   \n",
    "    lenta = [] \n",
    "    \n",
    "    for element in elements:\n",
    "       \n",
    "        try:\n",
    "            lenta.append({\n",
    "    \n",
    "            'title': str(element.find('div', class_= 'sku-card-small__title').get_text(strip = True)), # selecting element data class\n",
    "            'price': str(element.find('div', class_= 'sku-price--regular').get_text(strip = True)).replace(\"₽\", \"\").replace(\" \", \"\"),\n",
    "            'price_discount': str(element.find('div', class_= 'sku-price--primary').get_text(strip = True)).replace(\"₽\", \"\").replace(\" \", \"\")\n",
    "            })\n",
    "          \n",
    "        except TypeError:\n",
    "                 print (\"Error\")\n",
    "\n",
    "    return lenta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_csv(elements, path):\n",
    "   \n",
    "    \"\"\"\n",
    "    write to file function\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['product', 'price', 'price_discount'])\n",
    "        for element in elements:\n",
    "             writer.writerow([element['title'], element['price'], element['price_discount']])\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "def parser():\n",
    "    \n",
    "    \"\"\" \n",
    "    functions parsing \n",
    "    \"\"\"\n",
    "    \n",
    "    PAGENATION = input('Specify the number of pages ')\n",
    "    PAGENATION = int(PAGENATION.strip())\n",
    "    html = get_html(URL)\n",
    "    \n",
    "    if html.status_code == 200:\n",
    "        lentas = []\n",
    "        for page in range(1, PAGENATION+1):  # here you can set the cycle for which pages to parse\n",
    "            print(f'Data parsing process: {page}')\n",
    "            html = get_html(URL, params={'page': page})\n",
    "            lentas.extend(get_data(html.text))\n",
    "            save_csv(lentas, CSV)\n",
    "        print('The parsing process is finished')    \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        print('Error parsing')\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser() # starting parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
